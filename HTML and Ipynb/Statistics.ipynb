{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Statistics.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RUUrrwYQNaR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Reading Statistics Papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7bJfOphNz2f",
        "colab_type": "code",
        "outputId": "1e717a95-d6d7-4e82-f4b0-26b9685c2b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## Read the statistics papers from 2016 to 2018 and store it in a file\n",
        "import urllib\n",
        "url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=stat&from=2016-01-01&until=2018-11-31&metadataPrefix=arXiv' \n",
        "data = urllib.request.urlopen(url).read()\n",
        "\n",
        "stat = open('stat1', 'wb')\n",
        "stat.write(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2032744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "c8sMHkfZN-e1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Extract the title and abstract from papers - Read from finance1 to finance2\n",
        "\n",
        "!xml_grep 'title|abstract' stat1  > stat2.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RR-pvgZXOD_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Remove Junk lines , here we remove first 3 lines and last 3 lines which are not necessary\n",
        "!cat stat2.txt | tail -n +4 | head -n -3 > stat3.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3VttntpOXqq",
        "colab_type": "code",
        "outputId": "fcba2f02-63ea-4679-f7d8-4c6db6fd94ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## Reading packages for Text classification\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, numpy, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "#import sklearn_crfsuite\n",
        "#from sklearn_crfsuite import scorers\n",
        "#from sklearn_crfsuite import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIJ88IP6Od_-",
        "colab_type": "code",
        "outputId": "21a17d55-d359-41ad-92fd-44d207c5ad10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Stopwords import and removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Alxw521Oi3U",
        "colab_type": "code",
        "outputId": "8331153a-eb1f-4f42-b207-29e0d0483d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "# load the dataset # dataset contains combined labels and text from all training papers\n",
        "data = open('labeled_sentences (1).txt').read()[:-2]\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    #print(content)\n",
        "    labels.append(content[0])\n",
        "    filtered_sentence = [w.lower() for w in content[1:] if not w in stopwords]\n",
        "    texts.append(filtered_sentence)\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "print(trainDF['label'].unique())\n",
        "trainDF.head(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MISC' 'AIMX' 'OWNX' 'CONT' 'BASE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[minimum, description, length, principle, onli...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[underlying, model, class, discrete,, total, e...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  [minimum, description, length, principle, onli...  MISC\n",
              "1  [underlying, model, class, discrete,, total, e...  MISC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "wNS2hA9nOqil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Used the obtained dataset for training\n",
        "train_x, valid1_x, train_y, valid1_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iI1uBVOOsSR",
        "colab_type": "code",
        "outputId": "ef54dbd1-56c9-498b-a7c6-9c41d36ec321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "## Convert from list to string\n",
        "tempp = []\n",
        "\n",
        "for item in train_x:\n",
        "    tempp.append(\" \".join(item))\n",
        "#print(len(train_x))\n",
        "\n",
        "#tempp1 =[]\n",
        "#for item1 in valid_x:\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    \n",
        "#print(len(tempp1))\n",
        "\n",
        "temp =[]\n",
        "temp_len=0\n",
        "for item2 in texts:\n",
        "    temp.append(\" \".join(item2))\n",
        "    temp_len = temp_len+len(texts)\n",
        "print(len(temp))\n",
        "print(temp_len)\n",
        "print(type(temp))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18627\n",
            "346965129\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IAiDB7xpOv8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(temp)\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(tempp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FcKRuowAOzRr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create a classifier\n",
        "import csv\n",
        "trainDF2 = pandas.DataFrame()  \n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    #std_clf = make_pipeline(StandardScaler(with_mean=False), TruncatedSVD(100), MultinominalNB())\n",
        "    #std_clf.fit(feature_vector_train, label)\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    #predictions = classifier.predict(feature_vector_valid)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return predictions\n",
        "    #tt = classifier.predict(feature_vector_valid)\n",
        "    #labels3 = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    #trainDF2['labels'] = labels3\n",
        "    #trainDF2['text']= valid_x\n",
        "    #print(trainDF2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqU9A8xPPAZR",
        "colab_type": "code",
        "outputId": "01117db9-bf11-40e9-918c-27df027e8ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Read title and abstracts and loop through them \n",
        "import re\n",
        "global_list = []\n",
        "title_list =[]\n",
        "\n",
        "test = open(\"stat3.txt\",'r').read().split(\"</abstract>\")\n",
        "#print(test[1])\n",
        "for idx,i in enumerate(test):\n",
        "  title = re.findall(r\"(?<=<title>).*(?=</title>)\",i.replace(\"\\n\",\"\"))\n",
        "  #print(title)\n",
        "  abstract = re.findall(r\"(?<=<abstract>).*\",i.replace(\"\\n\",\"\"))\n",
        "  #print(abstract[0].replace(\"\\n\",\"\"))\n",
        "  nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",abstract[0].replace('\"',\"\").replace('\\n',''))\n",
        "  #temp_abs = re.sub(r\"((?<=[^i]\\.)|\\.(?=[^e]))\",\"\\n\",abstract[0])\n",
        "  #print(abstract)\n",
        "  #temp_str = temp_abs.split(\"\\n\")\n",
        "  #print(temp_str[0])\n",
        "  #print(nlist[1])\n",
        "  global_list.append(nlist)\n",
        "  title_list.append(title)\n",
        "  #print(global_list)\n",
        " \n",
        "  if idx >50:\n",
        "    #print(global_list)\n",
        "    break\n",
        "  #print(abstract[0])\n",
        "  #nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",str(abstract))\n",
        "  \n",
        "  #print(nlist[1])\n",
        "  \n",
        "  #tempp1 =[]\n",
        "  '''\n",
        "  for idx, item1 in enumerate(nlist):\n",
        "    \n",
        "    if idx > 1 :\n",
        "      break;\n",
        "      print(item1)\n",
        "      tempp1.append(\" \".join(item1))\n",
        "    #print(tempp1)  \n",
        "    \n",
        "    xvalid_count =  count_vect.transform(tempp1)\n",
        "    for item in nlist:\n",
        "      print(item)\n",
        "      valid_x = item\n",
        "      #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "    \n",
        "  '''\n",
        "  #print(global_list[0])\n",
        "  #print(global_list[1])\n",
        "  #print(global_list[2])\n",
        "  #for idx, item1 in enumerate(global_list) :\n",
        "  #  if idx > 1:\n",
        "  #    break\n",
        "  #  print(item1)\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    #xvalid_count =  count_vect.transform(tempp1)\n",
        "    #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HePYn1kpPFeR",
        "colab_type": "code",
        "outputId": "d73f9e35-07f3-4fcf-fea2-23ecf09c4f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "## Print triples from data\n",
        "\n",
        "#print(global_list[1])\n",
        "for idx, (item, title) in enumerate(zip(global_list, title_list)):\n",
        "  \n",
        "    \n",
        "\n",
        "  #print(item)\n",
        "  valid_x = item\n",
        "  xvalid_count =  count_vect.transform(valid_x)\n",
        "  accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "  #print(\"\\n\\n\")\n",
        "  if idx>1:\n",
        "    break\n",
        "  \n",
        "  title_id = hash(str(title))\n",
        "  abstract_id = hash(str(item))\n",
        "  line1 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://xmlns.com/foaf/0.1/name>\" + '\"' + \" \".join(title) + '\"' +\".\"\n",
        "  line2 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/\" + str(abstract_id)+ \">\"\n",
        "  line3 = \"<https://w3id.org/skg/articles/\" + str(abstract_id) +\"><http://purl.org/dc/terms/abstract/text>\" + '\"' + \" \".join(item) + '\"'\n",
        "  print(line1,line2,line3,sep =\"\\n\")\n",
        "  for acc,element in zip(accuracy,item):\n",
        "    print('<http://purl.org/dc/terms/abstract/{} > \"{}\"'.format(acc, element))  \n",
        "    #line4 = (\"<http://purl.org/dc/terms/abstract/\" + str(acc) + \">\" + '\"' + str(element) + '\"' )\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<https://w3id.org/skg/articles/-7734700797018972234> <http://xmlns.com/foaf/0.1/name>\"Reduced bias nonparametric lifetime density and hazard estimation\".\n",
            "<https://w3id.org/skg/articles/-7734700797018972234> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/8690684252333690204>\n",
            "<https://w3id.org/skg/articles/8690684252333690204><http://purl.org/dc/terms/abstract/text>\"  Kernel-based nonparametric hazard rate estimation is considered with aspecial class of infinite-order kernels that achieves favorable bias and meansquare error properties  A fully automatic and adaptive implementation of adensity and hazard rate estimator is proposed for randomly right censored data Careful selection of the bandwidth in the proposed estimators yields estimatesthat are more efficient in terms of overall mean squared error performance, andin some cases achieves a nearly parametric convergence rate  Additionally,rapidly converging bandwidth estimates are presented for use in second-orderkernels to supplement such kernel-based methods in hazard rate estimation Simulations illustrate the improved accuracy of the proposed estimator againstother nonparametric estimators of the density and hazard function  A real dataapplication is also presented on survival data from 13,166 breast carcinomapatients.\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \"  Kernel-based nonparametric hazard rate estimation is considered with aspecial class of infinite-order kernels that achieves favorable bias and meansquare error properties\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" A fully automatic and adaptive implementation of adensity and hazard rate estimator is proposed for randomly right censored data\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"Careful selection of the bandwidth in the proposed estimators yields estimatesthat are more efficient in terms of overall mean squared error performance, andin some cases achieves a nearly parametric convergence rate\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Additionally,rapidly converging bandwidth estimates are presented for use in second-orderkernels to supplement such kernel-based methods in hazard rate estimation\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"Simulations illustrate the improved accuracy of the proposed estimator againstother nonparametric estimators of the density and hazard function\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" A real dataapplication is also presented on survival data from 13,166 breast carcinomapatients.\"\n",
            "<https://w3id.org/skg/articles/1711393881944931099> <http://xmlns.com/foaf/0.1/name>\"Quantile and Probability Curves Without Crossing\".\n",
            "<https://w3id.org/skg/articles/1711393881944931099> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-2721053281257085775>\n",
            "<https://w3id.org/skg/articles/-2721053281257085775><http://purl.org/dc/terms/abstract/text>\"  This paper proposes a method to address the longstanding problem of lack ofmonotonicity in estimation of conditional and structural quantile functions,also known as the quantile crossing problem  The method consists in sorting ormonotone rearranging the original estimated non-monotone curve into a monotonerearranged curve  We show that the rearranged curve is closer to the truequantile curve in finite samples than the original curve, establish afunctional delta method for rearrangement-related operators, and derivefunctional limit theory for the entire rearranged curve and its functionals  Wealso establish validity of the bootstrap for estimating the limit law of thethe entire rearranged curve and its functionals  Our limit results are genericin that they apply to every estimator of a monotone econometric function,provided that the estimator satisfies a functional central limit theorem andthe function satisfies some smoothness conditions  Consequently, our resultsapply to estimation of other econometric functions with monotonicityrestrictions, such as demand, production, distribution, and structuraldistribution functions  We illustrate the results with an application toestimation of structural quantile functions using data on Vietnam veteranstatus and earnings.\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"  This paper proposes a method to address the longstanding problem of lack ofmonotonicity in estimation of conditional and structural quantile functions,also known as the quantile crossing problem\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" The method consists in sorting ormonotone rearranging the original estimated non-monotone curve into a monotonerearranged curve\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We show that the rearranged curve is closer to the truequantile curve in finite samples than the original curve, establish afunctional delta method for rearrangement-related operators, and derivefunctional limit theory for the entire rearranged curve and its functionals\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Wealso establish validity of the bootstrap for estimating the limit law of thethe entire rearranged curve and its functionals\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Our limit results are genericin that they apply to every estimator of a monotone econometric function,provided that the estimator satisfies a functional central limit theorem andthe function satisfies some smoothness conditions\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" Consequently, our resultsapply to estimation of other econometric functions with monotonicityrestrictions, such as demand, production, distribution, and structuraldistribution functions\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We illustrate the results with an application toestimation of structural quantile functions using data on Vietnam veteranstatus and earnings.\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}