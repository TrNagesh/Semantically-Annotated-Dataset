{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Math.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"eJqdvkVLWnFm","colab_type":"code","colab":{}},"cell_type":"code","source":["## Reading Math Papers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YXFhBQSHXBYd","colab_type":"code","outputId":"9c6fb976-3a68-4583-9c30-a7ef0d4b8f69","executionInfo":{"status":"ok","timestamp":1543728554456,"user_tz":-60,"elapsed":14618,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["## Read the math papers from 2016 to 2018 and store it in a file\n","import urllib\n","url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=math&from=2016-01-01&until=2018-11-31&metadataPrefix=arXiv' \n","data = urllib.request.urlopen(url).read()\n","\n","m = open('math1', 'wb')\n","m.write(data)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1692561"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"Cb_K5UcAXLen","colab_type":"code","colab":{}},"cell_type":"code","source":["## Extract the title and abstract from papers - Read from finance1 to finance2\n","!xml_grep 'title|abstract' math1  > math2.txt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cMA99CyHXSVu","colab_type":"code","colab":{}},"cell_type":"code","source":["## Remove Junk lines , here we remove first 3 lines and last 3 lines which are not necessary\n","!cat math2.txt | tail -n +4 | head -n -3 > math3.txt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VpUT3OXSXZy8","colab_type":"code","colab":{}},"cell_type":"code","source":["## Reading packages for Text classification\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","\n","import pandas, numpy, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import sklearn\n","#import sklearn_crfsuite\n","#from sklearn_crfsuite import scorers\n","#from sklearn_crfsuite import metrics\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1jI6p6DoXebT","colab_type":"code","outputId":"be0bb310-76e4-4523-f326-eb39d9b1d4d4","executionInfo":{"status":"ok","timestamp":1543728585389,"user_tz":-60,"elapsed":567,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["## Stopwords import and removal\n","import nltk\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","stopwords = set(stopwords.words('english'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"c3yoeSaZXiJD","colab_type":"code","outputId":"27200e6b-c48d-4c64-983e-e74f1062b773","executionInfo":{"status":"ok","timestamp":1543728587693,"user_tz":-60,"elapsed":811,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"cell_type":"code","source":["# load the dataset # dataset contains combined labels and text from all training papers\n","data = open('labeled_sentences (1).txt').read()[:-2]\n","labels, texts = [], []\n","for i, line in enumerate(data.split(\"\\n\")):\n","    content = line.split()\n","    #print(content)\n","    labels.append(content[0])\n","    filtered_sentence = [w.lower() for w in content[1:] if not w in stopwords]\n","    texts.append(filtered_sentence)\n","\n","# create a dataframe using texts and lables\n","trainDF = pandas.DataFrame()\n","trainDF['text'] = texts\n","trainDF['label'] = labels\n","print(trainDF['label'].unique())\n","trainDF.head(2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['MISC' 'AIMX' 'OWNX' 'CONT' 'BASE']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[minimum, description, length, principle, onli...</td>\n","      <td>MISC</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[underlying, model, class, discrete,, total, e...</td>\n","      <td>MISC</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text label\n","0  [minimum, description, length, principle, onli...  MISC\n","1  [underlying, model, class, discrete,, total, e...  MISC"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"yrmW-9IuXlX7","colab_type":"code","colab":{}},"cell_type":"code","source":["## Used the obtained dataset for training\n","train_x, valid1_x, train_y, valid1_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xYODx1HEXpXq","colab_type":"code","outputId":"cc76b50b-dc0d-4d24-9821-534d5d432c64","executionInfo":{"status":"ok","timestamp":1543728597525,"user_tz":-60,"elapsed":599,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["## Convert from list to string\n","tempp = []\n","\n","for item in train_x:\n","    tempp.append(\" \".join(item))\n","#print(len(train_x))\n","\n","#tempp1 =[]\n","#for item1 in valid_x:\n","    #tempp1.append(\" \".join(item1))\n","    \n","#print(len(tempp1))\n","\n","temp =[]\n","temp_len=0\n","for item2 in texts:\n","    temp.append(\" \".join(item2))\n","    temp_len = temp_len+len(texts)\n","print(len(temp))\n","print(temp_len)\n","print(type(temp))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["19162\n","367182244\n","<class 'list'>\n"],"name":"stdout"}]},{"metadata":{"id":"IduY-GUPXsoJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# create a count vectorizer object \n","count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","count_vect.fit(temp)\n","\n","# transform the training and validation data using count vectorizer object\n","xtrain_count =  count_vect.transform(tempp)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T6IHefGSXwDz","colab_type":"code","colab":{}},"cell_type":"code","source":["## Create a classifier\n","import csv\n","trainDF2 = pandas.DataFrame()  \n","\n","def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n","    # fit the training dataset on the classifier\n","    #std_clf = make_pipeline(StandardScaler(with_mean=False), TruncatedSVD(100), MultinominalNB())\n","    #std_clf.fit(feature_vector_train, label)\n","    classifier.fit(feature_vector_train, label)\n","    \n","    # predict the labels on validation dataset\n","    #predictions = classifier.predict(feature_vector_valid)\n","    predictions = classifier.predict(feature_vector_valid)\n","    return predictions\n","    #tt = classifier.predict(feature_vector_valid)\n","    #labels3 = classifier.predict(feature_vector_valid)\n","    \n","    #trainDF2['labels'] = labels3\n","    #trainDF2['text']= valid_x\n","    #print(trainDF2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fcCZvQqMXysk","colab_type":"code","outputId":"52c0b361-4aac-47d7-a65f-3501b4bbffd3","executionInfo":{"status":"ok","timestamp":1543728624436,"user_tz":-60,"elapsed":514,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["## Read title and abstracts and loop through them \n","import re\n","global_list = []\n","title_list =[]\n","\n","test = open(\"math3.txt\",'r').read().split(\"</abstract>\")\n","#print(test[1])\n","for idx,i in enumerate(test):\n","  title = re.findall(r\"(?<=<title>).*(?=</title>)\",i.replace(\"\\n\",\"\"))\n","  #print(title)\n","  abstract = re.findall(r\"(?<=<abstract>).*\",i.replace(\"\\n\",\"\"))\n","  #print(abstract[0].replace(\"\\n\",\"\"))\n","  nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",abstract[0].replace('\"',\"\").replace('\\n',''))\n","  #temp_abs = re.sub(r\"((?<=[^i]\\.)|\\.(?=[^e]))\",\"\\n\",abstract[0])\n","  #print(abstract)\n","  #temp_str = temp_abs.split(\"\\n\")\n","  #print(temp_str[0])\n","  #print(nlist[1])\n","  global_list.append(nlist)\n","  title_list.append(title)\n","  #print(global_list)\n"," \n","  if idx >50:\n","    #print(global_list)\n","    break\n","  #print(abstract[0])\n","  #nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",str(abstract))\n","  \n","  #print(nlist[1])\n","  \n","  #tempp1 =[]\n","  '''\n","  for idx, item1 in enumerate(nlist):\n","    \n","    if idx > 1 :\n","      break;\n","      print(item1)\n","      tempp1.append(\" \".join(item1))\n","    #print(tempp1)  \n","    \n","    xvalid_count =  count_vect.transform(tempp1)\n","    for item in nlist:\n","      print(item)\n","      valid_x = item\n","      #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","    \n","  '''\n","  #print(global_list[0])\n","  #print(global_list[1])\n","  #print(global_list[2])\n","  #for idx, item1 in enumerate(global_list) :\n","  #  if idx > 1:\n","  #    break\n","  #  print(item1)\n","    #tempp1.append(\" \".join(item1))\n","    #xvalid_count =  count_vect.transform(tempp1)\n","    #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n","  return _compile(pattern, flags).split(string, maxsplit)\n"],"name":"stderr"}]},{"metadata":{"id":"qrrKt1MuX18e","colab_type":"code","outputId":"ea18aa9b-3cdf-455b-807e-0ea8a74d82f9","executionInfo":{"status":"ok","timestamp":1543728636906,"user_tz":-60,"elapsed":4464,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"cell_type":"code","source":["## Print triples from data\n","\n","#print(global_list[1])\n","for idx, (item, title) in enumerate(zip(global_list, title_list)):\n","  \n","    \n","\n","  #print(item)\n","  valid_x = item\n","  xvalid_count =  count_vect.transform(valid_x)\n","  accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n","  #print(\"\\n\\n\")\n","  if idx>1:\n","    break\n","  \n","  title_id = hash(str(title))\n","  abstract_id = hash(str(item))\n","  line1 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://xmlns.com/foaf/0.1/name>\" + '\"' + \" \".join(title) + '\"' +\".\"\n","  line2 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/\" + str(abstract_id)+ \">\"\n","  line3 = \"<https://w3id.org/skg/articles/\" + str(abstract_id) +\"><http://purl.org/dc/terms/abstract/text>\" + '\"' + \" \".join(item) + '\"'\n","  print(line1,line2,line3,sep =\"\\n\")\n","  for acc,element in zip(accuracy,item):\n","    print('<http://purl.org/dc/terms/abstract/{} > \"{}\"'.format(acc, element))  \n","    #line4 = (\"<http://purl.org/dc/terms/abstract/\" + str(acc) + \">\" + '\"' + str(element) + '\"' )\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["<https://w3id.org/skg/articles/-2688487514614604503> <http://xmlns.com/foaf/0.1/name>\"Monoid generalizations of the Richard Thompson groups\".\n","<https://w3id.org/skg/articles/-2688487514614604503> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-4269569602989116255>\n","<https://w3id.org/skg/articles/-4269569602989116255><http://purl.org/dc/terms/abstract/text>\"  The groups G_{k,1} of Richard Thompson and Graham Higman can be generalizedin a natural way to monoids, that we call M_{k,1}, and to inverse monoids,called Inv_{k,1}; this is done by simply generalizing bijections to partialfunctions or partial injective functions  The monoids M_{k,1} have connectionswith circuit complexity (studied in another paper)  Here we prove that M_{k,1}and Inv_{k,1} are congruence-simple for all k  Their Green relations J and Dare characterized: M_{k,1} and Inv_{k,1} are J-0-simple, and they have k-1non-zero D-classes  They are submonoids of the multiplicative part of the Cuntzalgebra O_k  They are finitely generated, and their word problem over anyfinite generating set is in P  Their word problem is coNP-complete over certaininfinite generating sets   Changes in this version: Section 4 has been thoroughly revised, and errorshave been corrected; however, the main results of Section 4 do not change Sections 1, 2, and 3 are unchanged, except for the proof of Theorem 2 3, whichwas incomplete; a complete proof was published in the Appendix of reference[6], and is also given here.\"\n","<http://purl.org/dc/terms/abstract/OWNX > \"  The groups G_{k,1} of Richard Thompson and Graham Higman can be generalizedin a natural way to monoids, that we call M_{k,1}, and to inverse monoids,called Inv_{k,1}; this is done by simply generalizing bijections to partialfunctions or partial injective functions\"\n","<http://purl.org/dc/terms/abstract/AIMX > \" The monoids M_{k,1} have connectionswith circuit complexity (studied in another paper)\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Here we prove that M_{k,1}and Inv_{k,1} are congruence-simple for all k\"\n","<http://purl.org/dc/terms/abstract/MISC > \" Their Green relations J and Dare characterized: M_{k,1} and Inv_{k,1} are J-0-simple, and they have k-1non-zero D-classes\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" They are submonoids of the multiplicative part of the Cuntzalgebra O_k\"\n","<http://purl.org/dc/terms/abstract/MISC > \" They are finitely generated, and their word problem over anyfinite generating set is in P\"\n","<http://purl.org/dc/terms/abstract/MISC > \" Their word problem is coNP-complete over certaininfinite generating sets\"\n","<http://purl.org/dc/terms/abstract/OWNX > \"  Changes in this version: Section 4 has been thoroughly revised, and errorshave been corrected; however, the main results of Section 4 do not change\"\n","<http://purl.org/dc/terms/abstract/OWNX > \"Sections 1, 2, and 3 are unchanged, except for the proof of Theorem 2\"\n","<http://purl.org/dc/terms/abstract/OWNX > \"3, whichwas incomplete; a complete proof was published in the Appendix of reference[6], and is also given here.\"\n","<https://w3id.org/skg/articles/5327473888016701964> <http://xmlns.com/foaf/0.1/name>\"Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo  Codes\".\n","<https://w3id.org/skg/articles/5327473888016701964> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-7335421883133534815>\n","<https://w3id.org/skg/articles/-7335421883133534815><http://purl.org/dc/terms/abstract/text>\"  It has been observed that particular rate-1/2 partially systematic parallelconcatenated convolutional codes (PCCCs) can achieve a lower error floor thanthat of their rate-1/3 parent codes  Nevertheless, good puncturing patterns canonly be identified by means of an exhaustive search, whilst convergence towardslow bit error probabilities can be problematic when the systematic output of arate-1/2 partially systematic PCCC is heavily punctured  In this paper, wepresent and study a family of rate-1/2 partially systematic PCCCs, which wecall pseudo-randomly punctured codes  We evaluate their bit error rateperformance and we show that they always yield a lower error floor than that oftheir rate-1/3 parent codes  Furthermore, we compare analytic results tosimulations and we demonstrate that their performance converges towards theerror floor region, owning to the moderate puncturing of their systematicoutput  Consequently, we propose pseudo-random puncturing as a means ofimproving the bandwidth efficiency of a PCCC and simultaneously lowering itserror floor.\"\n","<http://purl.org/dc/terms/abstract/MISC > \"  It has been observed that particular rate-1/2 partially systematic parallelconcatenated convolutional codes (PCCCs) can achieve a lower error floor thanthat of their rate-1/3 parent codes\"\n","<http://purl.org/dc/terms/abstract/MISC > \" Nevertheless, good puncturing patterns canonly be identified by means of an exhaustive search, whilst convergence towardslow bit error probabilities can be problematic when the systematic output of arate-1/2 partially systematic PCCC is heavily punctured\"\n","<http://purl.org/dc/terms/abstract/AIMX > \" In this paper, wepresent and study a family of rate-1/2 partially systematic PCCCs, which wecall pseudo-randomly punctured codes\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" We evaluate their bit error rateperformance and we show that they always yield a lower error floor than that oftheir rate-1/3 parent codes\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Furthermore, we compare analytic results tosimulations and we demonstrate that their performance converges towards theerror floor region, owning to the moderate puncturing of their systematicoutput\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Consequently, we propose pseudo-random puncturing as a means ofimproving the bandwidth efficiency of a PCCC and simultaneously lowering itserror floor.\"\n"],"name":"stdout"}]}]}