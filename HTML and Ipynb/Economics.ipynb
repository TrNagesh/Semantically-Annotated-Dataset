{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Economics.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RUUrrwYQNaR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Reading Economics Papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7bJfOphNz2f",
        "colab_type": "code",
        "outputId": "03636456-30d4-40e4-f986-5657ccd95206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## Read the economics papers from 2016 to 2018 and store it in a file\n",
        "import urllib\n",
        "url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=econ&from=2016-01-01&until=2018-11-31&metadataPrefix=arXiv' \n",
        "data = urllib.request.urlopen(url).read()\n",
        "\n",
        "ee = open('eco1', 'wb')\n",
        "ee.write(data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1372062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "c8sMHkfZN-e1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Extract the title and abstract from papers - Read from finance1 to finance2\n",
        "\n",
        "!xml_grep 'title|abstract' eco1  > eco2.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RR-pvgZXOD_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Remove Junk lines , here we remove first 3 lines and last 3 lines which are not necessary\n",
        "!cat eco2.txt | tail -n +4 | head -n -3 > eco3.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3VttntpOXqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "344d6a3d-bb08-43c1-dd7a-8f55b8564bef"
      },
      "cell_type": "code",
      "source": [
        "## Reading packages for Text classification\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, numpy, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "#import sklearn_crfsuite\n",
        "#from sklearn_crfsuite import scorers\n",
        "#from sklearn_crfsuite import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIJ88IP6Od_-",
        "colab_type": "code",
        "outputId": "b8912097-11cd-4ea3-f37c-3293a81b89aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Stopwords import and removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Alxw521Oi3U",
        "colab_type": "code",
        "outputId": "98b81a46-997e-4e5e-b539-745868acfb3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "# load the dataset # dataset contains combined labels and text from all training papers\n",
        "data = open('labeled_sentences (1).txt').read()[:-2]\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    #print(content)\n",
        "    labels.append(content[0])\n",
        "    filtered_sentence = [w.lower() for w in content[1:] if not w in stopwords]\n",
        "    texts.append(filtered_sentence)\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "print(trainDF['label'].unique())\n",
        "trainDF.head(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MISC' 'AIMX' 'OWNX' 'CONT' 'BASE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[minimum, description, length, principle, onli...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[underlying, model, class, discrete,, total, e...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  [minimum, description, length, principle, onli...  MISC\n",
              "1  [underlying, model, class, discrete,, total, e...  MISC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "wNS2hA9nOqil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Used the obtained dataset for training\n",
        "train_x, valid1_x, train_y, valid1_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iI1uBVOOsSR",
        "colab_type": "code",
        "outputId": "6f4fbc62-4fc7-40eb-fba5-1d4d9c56edff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "## Convert from list to string\n",
        "tempp = []\n",
        "\n",
        "for item in train_x:\n",
        "    tempp.append(\" \".join(item))\n",
        "#print(len(train_x))\n",
        "\n",
        "#tempp1 =[]\n",
        "#for item1 in valid_x:\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    \n",
        "#print(len(tempp1))\n",
        "\n",
        "temp =[]\n",
        "temp_len=0\n",
        "for item2 in texts:\n",
        "    temp.append(\" \".join(item2))\n",
        "    temp_len = temp_len+len(texts)\n",
        "print(len(temp))\n",
        "print(temp_len)\n",
        "print(type(temp))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18627\n",
            "346965129\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IAiDB7xpOv8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(temp)\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(tempp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FcKRuowAOzRr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create a classifier\n",
        "import csv\n",
        "trainDF2 = pandas.DataFrame()  \n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    #std_clf = make_pipeline(StandardScaler(with_mean=False), TruncatedSVD(100), MultinominalNB())\n",
        "    #std_clf.fit(feature_vector_train, label)\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    #predictions = classifier.predict(feature_vector_valid)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return predictions\n",
        "    #tt = classifier.predict(feature_vector_valid)\n",
        "    #labels3 = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    #trainDF2['labels'] = labels3\n",
        "    #trainDF2['text']= valid_x\n",
        "    #print(trainDF2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqU9A8xPPAZR",
        "colab_type": "code",
        "outputId": "1e747680-458a-4006-feb2-028bcb2d56e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Read title and abstracts and loop through them \n",
        "import re\n",
        "global_list = []\n",
        "title_list =[]\n",
        "\n",
        "test = open(\"eco3.txt\",'r').read().split(\"</abstract>\")\n",
        "#print(test[1])\n",
        "for idx,i in enumerate(test):\n",
        "  title = re.findall(r\"(?<=<title>).*(?=</title>)\",i.replace(\"\\n\",\"\"))\n",
        "  #print(title)\n",
        "  abstract = re.findall(r\"(?<=<abstract>).*\",i.replace(\"\\n\",\"\"))\n",
        "  #print(abstract[0].replace(\"\\n\",\"\"))\n",
        "  nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",abstract[0].replace('\"',\"\").replace('\\n',''))\n",
        "  #temp_abs = re.sub(r\"((?<=[^i]\\.)|\\.(?=[^e]))\",\"\\n\",abstract[0])\n",
        "  #print(abstract)\n",
        "  #temp_str = temp_abs.split(\"\\n\")\n",
        "  #print(temp_str[0])\n",
        "  #print(nlist[1])\n",
        "  global_list.append(nlist)\n",
        "  title_list.append(title)\n",
        "  #print(global_list)\n",
        " \n",
        "  if idx >50:\n",
        "    #print(global_list)\n",
        "    break\n",
        "  #print(abstract[0])\n",
        "  #nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",str(abstract))\n",
        "  \n",
        "  #print(nlist[1])\n",
        "  \n",
        "  #tempp1 =[]\n",
        "  '''\n",
        "  for idx, item1 in enumerate(nlist):\n",
        "    \n",
        "    if idx > 1 :\n",
        "      break;\n",
        "      print(item1)\n",
        "      tempp1.append(\" \".join(item1))\n",
        "    #print(tempp1)  \n",
        "    \n",
        "    xvalid_count =  count_vect.transform(tempp1)\n",
        "    for item in nlist:\n",
        "      print(item)\n",
        "      valid_x = item\n",
        "      #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "    \n",
        "  '''\n",
        "  #print(global_list[0])\n",
        "  #print(global_list[1])\n",
        "  #print(global_list[2])\n",
        "  #for idx, item1 in enumerate(global_list) :\n",
        "  #  if idx > 1:\n",
        "  #    break\n",
        "  #  print(item1)\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    #xvalid_count =  count_vect.transform(tempp1)\n",
        "    #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HePYn1kpPFeR",
        "colab_type": "code",
        "outputId": "e6843dc5-e7fe-49d3-bbde-0d2fea83eb44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "## Print triples from data\n",
        "\n",
        "#print(global_list[1])\n",
        "for idx, (item, title) in enumerate(zip(global_list, title_list)):\n",
        "  \n",
        "    \n",
        "\n",
        "  #print(item)\n",
        "  valid_x = item\n",
        "  xvalid_count =  count_vect.transform(valid_x)\n",
        "  accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "  #print(\"\\n\\n\")\n",
        "  if idx>1:\n",
        "    break\n",
        "  \n",
        "  title_id = hash(str(title))\n",
        "  abstract_id = hash(str(item))\n",
        "  line1 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://xmlns.com/foaf/0.1/name>\" + '\"' + \" \".join(title) + '\"' +\".\"\n",
        "  line2 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/\" + str(abstract_id)+ \">\"\n",
        "  line3 = \"<https://w3id.org/skg/articles/\" + str(abstract_id) +\"><http://purl.org/dc/terms/abstract/text>\" + '\"' + \" \".join(item) + '\"'\n",
        "  print(line1,line2,line3,sep =\"\\n\")\n",
        "  for acc,element in zip(accuracy,item):\n",
        "    print('<http://purl.org/dc/terms/abstract/{} > \"{}\"'.format(acc, element))  \n",
        "    #line4 = (\"<http://purl.org/dc/terms/abstract/\" + str(acc) + \">\" + '\"' + str(element) + '\"' )\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<https://w3id.org/skg/articles/8094898672661697709> <http://xmlns.com/foaf/0.1/name>\"Quantile and Probability Curves Without Crossing\".\n",
            "<https://w3id.org/skg/articles/8094898672661697709> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-7751916543779414938>\n",
            "<https://w3id.org/skg/articles/-7751916543779414938><http://purl.org/dc/terms/abstract/text>\"  This paper proposes a method to address the longstanding problem of lack ofmonotonicity in estimation of conditional and structural quantile functions,also known as the quantile crossing problem  The method consists in sorting ormonotone rearranging the original estimated non-monotone curve into a monotonerearranged curve  We show that the rearranged curve is closer to the truequantile curve in finite samples than the original curve, establish afunctional delta method for rearrangement-related operators, and derivefunctional limit theory for the entire rearranged curve and its functionals  Wealso establish validity of the bootstrap for estimating the limit law of thethe entire rearranged curve and its functionals  Our limit results are genericin that they apply to every estimator of a monotone econometric function,provided that the estimator satisfies a functional central limit theorem andthe function satisfies some smoothness conditions  Consequently, our resultsapply to estimation of other econometric functions with monotonicityrestrictions, such as demand, production, distribution, and structuraldistribution functions  We illustrate the results with an application toestimation of structural quantile functions using data on Vietnam veteranstatus and earnings.\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"  This paper proposes a method to address the longstanding problem of lack ofmonotonicity in estimation of conditional and structural quantile functions,also known as the quantile crossing problem\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" The method consists in sorting ormonotone rearranging the original estimated non-monotone curve into a monotonerearranged curve\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We show that the rearranged curve is closer to the truequantile curve in finite samples than the original curve, establish afunctional delta method for rearrangement-related operators, and derivefunctional limit theory for the entire rearranged curve and its functionals\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Wealso establish validity of the bootstrap for estimating the limit law of thethe entire rearranged curve and its functionals\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Our limit results are genericin that they apply to every estimator of a monotone econometric function,provided that the estimator satisfies a functional central limit theorem andthe function satisfies some smoothness conditions\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" Consequently, our resultsapply to estimation of other econometric functions with monotonicityrestrictions, such as demand, production, distribution, and structuraldistribution functions\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We illustrate the results with an application toestimation of structural quantile functions using data on Vietnam veteranstatus and earnings.\"\n",
            "<https://w3id.org/skg/articles/-8455937720046133964> <http://xmlns.com/foaf/0.1/name>\"Improving Estimates of Monotone Functions by Rearrangement\".\n",
            "<https://w3id.org/skg/articles/-8455937720046133964> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-3755805548760784824>\n",
            "<https://w3id.org/skg/articles/-3755805548760784824><http://purl.org/dc/terms/abstract/text>\"  Suppose that a target function is monotonic, namely, weakly increasing, andan original estimate of the target function is available, which is not weaklyincreasing  Many common estimation methods used in statistics produce suchestimates  We show that these estimates can always be improved with no harmusing rearrangement techniques: The rearrangement methods, univariate andmultivariate, transform the original estimate to a monotonic estimate, and theresulting estimate is closer to the true curve in common metrics than theoriginal estimate  We illustrate the results with a computational example andan empirical example dealing with age-height growth charts.\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"  Suppose that a target function is monotonic, namely, weakly increasing, andan original estimate of the target function is available, which is not weaklyincreasing\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" Many common estimation methods used in statistics produce suchestimates\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We show that these estimates can always be improved with no harmusing rearrangement techniques: The rearrangement methods, univariate andmultivariate, transform the original estimate to a monotonic estimate, and theresulting estimate is closer to the true curve in common metrics than theoriginal estimate\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" We illustrate the results with a computational example andan empirical example dealing with age-height growth charts.\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}