{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Electrical.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RUUrrwYQNaR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Reading Electrical Papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7bJfOphNz2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f3c4318-2213-42ca-fb15-b48960875f72"
      },
      "cell_type": "code",
      "source": [
        "## Read the math papers from 2016 to 2018 and store it in a file\n",
        "import urllib\n",
        "url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=eess&from=2016-01-01&until=2018-11-31&metadataPrefix=arXiv' \n",
        "data = urllib.request.urlopen(url).read()\n",
        "\n",
        "e = open('ele1', 'wb')\n",
        "e.write(data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2082009"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "c8sMHkfZN-e1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Extract the title and abstract from papers - Read from finance1 to finance2\n",
        "!apt-get install xml-twig-tools\n",
        "!xml_grep 'title|abstract' ele1  > ele2.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RR-pvgZXOD_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Remove Junk lines , here we remove first 3 lines and last 3 lines which are not necessary\n",
        "!cat ele2.txt | tail -n +4 | head -n -3 > ele3.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3VttntpOXqq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Reading packages for Text classification\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, numpy, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "#import sklearn_crfsuite\n",
        "#from sklearn_crfsuite import scorers\n",
        "#from sklearn_crfsuite import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FIJ88IP6Od_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a4d8228-4b10-490c-f462-720e5b2d1f27"
      },
      "cell_type": "code",
      "source": [
        "## Stopwords import and removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Alxw521Oi3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "b361a422-0fd9-40b3-e27a-7427a3398d53"
      },
      "cell_type": "code",
      "source": [
        "# load the dataset # dataset contains combined labels and text from all training papers\n",
        "data = open('labeled_sentences (1).txt').read()[:-2]\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    #print(content)\n",
        "    labels.append(content[0])\n",
        "    filtered_sentence = [w.lower() for w in content[1:] if not w in stopwords]\n",
        "    texts.append(filtered_sentence)\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "print(trainDF['label'].unique())\n",
        "trainDF.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MISC' 'AIMX' 'OWNX' 'CONT' 'BASE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[minimum, description, length, principle, onli...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[underlying, model, class, discrete,, total, e...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  [minimum, description, length, principle, onli...  MISC\n",
              "1  [underlying, model, class, discrete,, total, e...  MISC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "wNS2hA9nOqil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Used the obtained dataset for training\n",
        "train_x, valid1_x, train_y, valid1_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iI1uBVOOsSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "127960bb-6603-4a87-9fbc-049619878175"
      },
      "cell_type": "code",
      "source": [
        "## Convert from list to string\n",
        "tempp = []\n",
        "\n",
        "for item in train_x:\n",
        "    tempp.append(\" \".join(item))\n",
        "#print(len(train_x))\n",
        "\n",
        "#tempp1 =[]\n",
        "#for item1 in valid_x:\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    \n",
        "#print(len(tempp1))\n",
        "\n",
        "temp =[]\n",
        "temp_len=0\n",
        "for item2 in texts:\n",
        "    temp.append(\" \".join(item2))\n",
        "    temp_len = temp_len+len(texts)\n",
        "print(len(temp))\n",
        "print(temp_len)\n",
        "print(type(temp))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18627\n",
            "346965129\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IAiDB7xpOv8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(temp)\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(tempp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FcKRuowAOzRr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create a classifier\n",
        "import csv\n",
        "trainDF2 = pandas.DataFrame()  \n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    #std_clf = make_pipeline(StandardScaler(with_mean=False), TruncatedSVD(100), MultinominalNB())\n",
        "    #std_clf.fit(feature_vector_train, label)\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    #predictions = classifier.predict(feature_vector_valid)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return predictions\n",
        "    #tt = classifier.predict(feature_vector_valid)\n",
        "    #labels3 = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    #trainDF2['labels'] = labels3\n",
        "    #trainDF2['text']= valid_x\n",
        "    #print(trainDF2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqU9A8xPPAZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c86d912f-70c9-4d96-bbbf-4f1eaf574609"
      },
      "cell_type": "code",
      "source": [
        "## Read title and abstracts and loop through them \n",
        "import re\n",
        "global_list = []\n",
        "title_list =[]\n",
        "\n",
        "test = open(\"ele3.txt\",'r').read().split(\"</abstract>\")\n",
        "#print(test[1])\n",
        "for idx,i in enumerate(test):\n",
        "  title = re.findall(r\"(?<=<title>).*(?=</title>)\",i.replace(\"\\n\",\"\"))\n",
        "  #print(title)\n",
        "  abstract = re.findall(r\"(?<=<abstract>).*\",i.replace(\"\\n\",\"\"))\n",
        "  #print(abstract[0].replace(\"\\n\",\"\"))\n",
        "  nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",abstract[0].replace('\"',\"\").replace('\\n',''))\n",
        "  #temp_abs = re.sub(r\"((?<=[^i]\\.)|\\.(?=[^e]))\",\"\\n\",abstract[0])\n",
        "  #print(abstract)\n",
        "  #temp_str = temp_abs.split(\"\\n\")\n",
        "  #print(temp_str[0])\n",
        "  #print(nlist[1])\n",
        "  global_list.append(nlist)\n",
        "  title_list.append(title)\n",
        "  #print(global_list)\n",
        " \n",
        "  if idx >50:\n",
        "    #print(global_list)\n",
        "    break\n",
        "  #print(abstract[0])\n",
        "  #nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",str(abstract))\n",
        "  \n",
        "  #print(nlist[1])\n",
        "  \n",
        "  #tempp1 =[]\n",
        "  '''\n",
        "  for idx, item1 in enumerate(nlist):\n",
        "    \n",
        "    if idx > 1 :\n",
        "      break;\n",
        "      print(item1)\n",
        "      tempp1.append(\" \".join(item1))\n",
        "    #print(tempp1)  \n",
        "    \n",
        "    xvalid_count =  count_vect.transform(tempp1)\n",
        "    for item in nlist:\n",
        "      print(item)\n",
        "      valid_x = item\n",
        "      #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "    \n",
        "  '''\n",
        "  #print(global_list[0])\n",
        "  #print(global_list[1])\n",
        "  #print(global_list[2])\n",
        "  #for idx, item1 in enumerate(global_list) :\n",
        "  #  if idx > 1:\n",
        "  #    break\n",
        "  #  print(item1)\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    #xvalid_count =  count_vect.transform(tempp1)\n",
        "    #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HePYn1kpPFeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "97e0b2f4-7211-4171-a914-03d8778e26ac"
      },
      "cell_type": "code",
      "source": [
        "## Print triples from data\n",
        "\n",
        "#print(global_list[1])\n",
        "for idx, (item, title) in enumerate(zip(global_list, title_list)):\n",
        "  \n",
        "    \n",
        "\n",
        "  #print(item)\n",
        "  valid_x = item\n",
        "  xvalid_count =  count_vect.transform(valid_x)\n",
        "  accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "  #print(\"\\n\\n\")\n",
        "  if idx>1:\n",
        "    break\n",
        "  \n",
        "  title_id = hash(str(title))\n",
        "  abstract_id = hash(str(item))\n",
        "  line1 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://xmlns.com/foaf/0.1/name>\" + '\"' + \" \".join(title) + '\"' +\".\"\n",
        "  line2 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/\" + str(abstract_id)+ \">\"\n",
        "  line3 = \"<https://w3id.org/skg/articles/\" + str(abstract_id) +\"><http://purl.org/dc/terms/abstract/text>\" + '\"' + \" \".join(item) + '\"'\n",
        "  print(line1,line2,line3,sep =\"\\n\")\n",
        "  for acc,element in zip(accuracy,item):\n",
        "    print('<http://purl.org/dc/terms/abstract/{} > \"{}\"'.format(acc, element))  \n",
        "    #line4 = (\"<http://purl.org/dc/terms/abstract/\" + str(acc) + \">\" + '\"' + str(element) + '\"' )\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<https://w3id.org/skg/articles/-6502291845625822428> <http://xmlns.com/foaf/0.1/name>\"Warping Peirce Quincuncial Panoramas\".\n",
            "<https://w3id.org/skg/articles/-6502291845625822428> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-6245615208384810736>\n",
            "<https://w3id.org/skg/articles/-6245615208384810736><http://purl.org/dc/terms/abstract/text>\"  The Peirce quincuncial projection is a mapping of the surface of a sphere tothe interior of a square  It is a conformal map except for four points on theequator  These points of non-conformality cause significant artifacts inphotographic applications  In this paper, we propose an algorithm anduser-interface to mitigate these artifacts  Moreover, in order to facilitate aninteractive user-interface, we present a fast algorithm for calculating thePeirce quincuncial projection of spherical imagery  We then promote the Peircequincuncial projection as a viable alternative to the more popularstereographic projection in some scenarios.\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"  The Peirce quincuncial projection is a mapping of the surface of a sphere tothe interior of a square\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" It is a conformal map except for four points on theequator\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" These points of non-conformality cause significant artifacts inphotographic applications\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" In this paper, we propose an algorithm anduser-interface to mitigate these artifacts\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Moreover, in order to facilitate aninteractive user-interface, we present a fast algorithm for calculating thePeirce quincuncial projection of spherical imagery\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We then promote the Peircequincuncial projection as a viable alternative to the more popularstereographic projection in some scenarios.\"\n",
            "<https://w3id.org/skg/articles/-2823227690419730964> <http://xmlns.com/foaf/0.1/name>\"Tracking Tetrahymena Pyriformis Cells using Decision Trees\".\n",
            "<https://w3id.org/skg/articles/-2823227690419730964> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/7463497532546105089>\n",
            "<https://w3id.org/skg/articles/7463497532546105089><http://purl.org/dc/terms/abstract/text>\"  Matching cells over time has long been the most difficult step in celltracking  In this paper, we approach this problem by recasting it as aclassification problem  We construct a feature set for each cell, and compute afeature difference vector between a cell in the current frame and a cell in aprevious frame  Then we determine whether the two cells represent the same cellover time by training decision trees as our binary classifiers  With the outputof decision trees, we are able to formulate an assignment problem for our cellassociation task and solve it using a modified version of the Hungarianalgorithm.\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \"  Matching cells over time has long been the most difficult step in celltracking\"\n",
            "<http://purl.org/dc/terms/abstract/AIMX > \" In this paper, we approach this problem by recasting it as aclassification problem\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We construct a feature set for each cell, and compute afeature difference vector between a cell in the current frame and a cell in aprevious frame\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" Then we determine whether the two cells represent the same cellover time by training decision trees as our binary classifiers\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" With the outputof decision trees, we are able to formulate an assignment problem for our cellassociation task and solve it using a modified version of the Hungarianalgorithm.\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}