{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ComputerScience.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"JcITrCpsZ1Mp","colab_type":"code","colab":{}},"cell_type":"code","source":["## Reading Computer Science Papers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eCyi3NP1aCfO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c57afee4-890a-4cf8-8dd7-63a77202a5c2","executionInfo":{"status":"ok","timestamp":1543728870376,"user_tz":-60,"elapsed":14143,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["## Read the computer science papers from 2016 to 2018 and store it in a file\n","import urllib\n","url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=cs&from=2015-11-01&until=2018-11-31&metadataPrefix=arXiv' \n","data = urllib.request.urlopen(url).read()\n","\n","f = open('computer1', 'wb')\n","f.write(data)"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2008844"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"B2CgIPlMaSY8","colab_type":"code","colab":{}},"cell_type":"code","source":["## Extract the title and abstract from papers - Read from finance1 to finance2\n","!xml_grep 'title|abstract' computer1  > computer2.txt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"khvGlr2waTRv","colab_type":"code","colab":{}},"cell_type":"code","source":["## Remove Junk lines , here we remove first 3 lines and last 3 lines which are not necessary\n","!cat computer2.txt | tail -n +4 | head -n -3 > computer3.txt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M86hugSAaWBp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"45f8080f-9dca-4b1d-ed44-98e6ff67d22c","executionInfo":{"status":"ok","timestamp":1543729081113,"user_tz":-60,"elapsed":2210,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["## Reading packages for Text classification\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","\n","import pandas, numpy, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import sklearn\n","#import sklearn_crfsuite\n","#from sklearn_crfsuite import scorers\n","#from sklearn_crfsuite import metrics\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"SxgrnH6iaZ41","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"3de5a861-8492-4c1c-b428-5e53f418e495","executionInfo":{"status":"ok","timestamp":1543729084375,"user_tz":-60,"elapsed":694,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["## Stopwords import and removal\n","import nltk\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","stopwords = set(stopwords.words('english'))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"HhwkmDlbae17","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"outputId":"1dbad1d7-28e7-4cea-9f67-d4fbaad8c1e4","executionInfo":{"status":"ok","timestamp":1543729088941,"user_tz":-60,"elapsed":752,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["# load the dataset # dataset contains combined labels and text from all training papers\n","data = open('labeled_sentences (1).txt').read()[:-2]\n","labels, texts = [], []\n","for i, line in enumerate(data.split(\"\\n\")):\n","    content = line.split()\n","    #print(content)\n","    labels.append(content[0])\n","    filtered_sentence = [w.lower() for w in content[1:] if not w in stopwords]\n","    texts.append(filtered_sentence)\n","\n","# create a dataframe using texts and lables\n","trainDF = pandas.DataFrame()\n","trainDF['text'] = texts\n","trainDF['label'] = labels\n","print(trainDF['label'].unique())\n","trainDF.head(2)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['MISC' 'AIMX' 'OWNX' 'CONT' 'BASE']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[minimum, description, length, principle, onli...</td>\n","      <td>MISC</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[underlying, model, class, discrete,, total, e...</td>\n","      <td>MISC</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text label\n","0  [minimum, description, length, principle, onli...  MISC\n","1  [underlying, model, class, discrete,, total, e...  MISC"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"CDEViDb8ahrk","colab_type":"code","colab":{}},"cell_type":"code","source":["## Used the obtained dataset for training\n","train_x, valid1_x, train_y, valid1_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DQDSdGkQalmk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"48f232f3-7ca6-46d1-82cb-2d893db7b62a","executionInfo":{"status":"ok","timestamp":1543729097079,"user_tz":-60,"elapsed":530,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["## Convert from list to string\n","tempp = []\n","\n","for item in train_x:\n","    tempp.append(\" \".join(item))\n","#print(len(train_x))\n","\n","#tempp1 =[]\n","#for item1 in valid_x:\n","    #tempp1.append(\" \".join(item1))\n","    \n","#print(len(tempp1))\n","\n","temp =[]\n","temp_len=0\n","for item2 in texts:\n","    temp.append(\" \".join(item2))\n","    temp_len = temp_len+len(texts)\n","print(len(temp))\n","print(temp_len)\n","print(type(temp))\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["19162\n","367182244\n","<class 'list'>\n"],"name":"stdout"}]},{"metadata":{"id":"p3J6gLLvaqyR","colab_type":"code","colab":{}},"cell_type":"code","source":["# create a count vectorizer object \n","count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","count_vect.fit(temp)\n","\n","# transform the training and validation data using count vectorizer object\n","xtrain_count =  count_vect.transform(tempp)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V-gbHfV1audz","colab_type":"code","colab":{}},"cell_type":"code","source":["## Create a classifier\n","import csv\n","trainDF2 = pandas.DataFrame()  \n","\n","def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n","    # fit the training dataset on the classifier\n","    #std_clf = make_pipeline(StandardScaler(with_mean=False), TruncatedSVD(100), MultinominalNB())\n","    #std_clf.fit(feature_vector_train, label)\n","    classifier.fit(feature_vector_train, label)\n","    \n","    # predict the labels on validation dataset\n","    #predictions = classifier.predict(feature_vector_valid)\n","    predictions = classifier.predict(feature_vector_valid)\n","    return predictions\n","    #tt = classifier.predict(feature_vector_valid)\n","    #labels3 = classifier.predict(feature_vector_valid)\n","    \n","    #trainDF2['labels'] = labels3\n","    #trainDF2['text']= valid_x\n","    #print(trainDF2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QxbQZbKkayA-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ae7034eb-c9ed-4ee1-fd28-8c8db62ceb87","executionInfo":{"status":"ok","timestamp":1543729120947,"user_tz":-60,"elapsed":647,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["## Read title and abstracts and loop through them \n","import re\n","global_list = []\n","title_list =[]\n","\n","test = open(\"computer3.txt\",'r').read().split(\"</abstract>\")\n","#print(test[1])\n","for idx,i in enumerate(test):\n","  title = re.findall(r\"(?<=<title>).*(?=</title>)\",i.replace(\"\\n\",\"\"))\n","  #print(title)\n","  abstract = re.findall(r\"(?<=<abstract>).*\",i.replace(\"\\n\",\"\"))\n","  #print(abstract[0].replace(\"\\n\",\"\"))\n","  nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",abstract[0].replace('\"',\"\").replace('\\n',''))\n","  #temp_abs = re.sub(r\"((?<=[^i]\\.)|\\.(?=[^e]))\",\"\\n\",abstract[0])\n","  #print(abstract)\n","  #temp_str = temp_abs.split(\"\\n\")\n","  #print(temp_str[0])\n","  #print(nlist[1])\n","  global_list.append(nlist)\n","  title_list.append(title)\n","  #print(global_list)\n"," \n","  if idx >50:\n","    #print(global_list)\n","    break\n","  #print(abstract[0])\n","  #nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",str(abstract))\n","  \n","  #print(nlist[1])\n","  \n","  #tempp1 =[]\n","  '''\n","  for idx, item1 in enumerate(nlist):\n","    \n","    if idx > 1 :\n","      break;\n","      print(item1)\n","      tempp1.append(\" \".join(item1))\n","    #print(tempp1)  \n","    \n","    xvalid_count =  count_vect.transform(tempp1)\n","    for item in nlist:\n","      print(item)\n","      valid_x = item\n","      #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","    \n","  '''\n","  #print(global_list[0])\n","  #print(global_list[1])\n","  #print(global_list[2])\n","  #for idx, item1 in enumerate(global_list) :\n","  #  if idx > 1:\n","  #    break\n","  #  print(item1)\n","    #tempp1.append(\" \".join(item1))\n","    #xvalid_count =  count_vect.transform(tempp1)\n","    #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","  "],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n","  return _compile(pattern, flags).split(string, maxsplit)\n"],"name":"stderr"}]},{"metadata":{"id":"7q1jlPwRa1UP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"outputId":"44732e1b-7c50-4fbc-e6f8-667fc4960c97","executionInfo":{"status":"ok","timestamp":1543729137125,"user_tz":-60,"elapsed":5649,"user":{"displayName":"nagesh tr","photoUrl":"https://lh3.googleusercontent.com/-8yQwPUFPAaU/AAAAAAAAAAI/AAAAAAAAAJs/4cYlCbm-6q8/s64/photo.jpg","userId":"01347380177990762495"}}},"cell_type":"code","source":["## Print triples from data\n","\n","#print(global_list[1])\n","for idx, (item, title) in enumerate(zip(global_list, title_list)):\n","  \n","    \n","\n","  #print(item)\n","  valid_x = item\n","  xvalid_count =  count_vect.transform(valid_x)\n","  accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n","  #print(\"\\n\\n\")\n","  if idx>2:\n","    break\n","  \n","  title_id = hash(str(title))\n","  abstract_id = hash(str(item))\n","  line1 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://xmlns.com/foaf/0.1/name>\" + '\"' + \" \".join(title) + '\"' +\".\"\n","  line2 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/\" + str(abstract_id)+ \">\"\n","  line3 = \"<https://w3id.org/skg/articles/\" + str(abstract_id) +\"><http://purl.org/dc/terms/abstract/text>\" + '\"' + \" \".join(item) + '\"'\n","  print(line1,line2,line3,sep =\"\\n\")\n","  for acc,element in zip(accuracy,item):\n","    print('<http://purl.org/dc/terms/abstract/{} > \"{}\"'.format(acc, element))  \n","    #line4 = (\"<http://purl.org/dc/terms/abstract/\" + str(acc) + \">\" + '\"' + str(element) + '\"' )\n","  "],"execution_count":13,"outputs":[{"output_type":"stream","text":["<https://w3id.org/skg/articles/-2867593985518337823> <http://xmlns.com/foaf/0.1/name>\"Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo  Codes\".\n","<https://w3id.org/skg/articles/-2867593985518337823> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-4392215306747285796>\n","<https://w3id.org/skg/articles/-4392215306747285796><http://purl.org/dc/terms/abstract/text>\"  It has been observed that particular rate-1/2 partially systematic parallelconcatenated convolutional codes (PCCCs) can achieve a lower error floor thanthat of their rate-1/3 parent codes  Nevertheless, good puncturing patterns canonly be identified by means of an exhaustive search, whilst convergence towardslow bit error probabilities can be problematic when the systematic output of arate-1/2 partially systematic PCCC is heavily punctured  In this paper, wepresent and study a family of rate-1/2 partially systematic PCCCs, which wecall pseudo-randomly punctured codes  We evaluate their bit error rateperformance and we show that they always yield a lower error floor than that oftheir rate-1/3 parent codes  Furthermore, we compare analytic results tosimulations and we demonstrate that their performance converges towards theerror floor region, owning to the moderate puncturing of their systematicoutput  Consequently, we propose pseudo-random puncturing as a means ofimproving the bandwidth efficiency of a PCCC and simultaneously lowering itserror floor.\"\n","<http://purl.org/dc/terms/abstract/MISC > \"  It has been observed that particular rate-1/2 partially systematic parallelconcatenated convolutional codes (PCCCs) can achieve a lower error floor thanthat of their rate-1/3 parent codes\"\n","<http://purl.org/dc/terms/abstract/MISC > \" Nevertheless, good puncturing patterns canonly be identified by means of an exhaustive search, whilst convergence towardslow bit error probabilities can be problematic when the systematic output of arate-1/2 partially systematic PCCC is heavily punctured\"\n","<http://purl.org/dc/terms/abstract/AIMX > \" In this paper, wepresent and study a family of rate-1/2 partially systematic PCCCs, which wecall pseudo-randomly punctured codes\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" We evaluate their bit error rateperformance and we show that they always yield a lower error floor than that oftheir rate-1/3 parent codes\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Furthermore, we compare analytic results tosimulations and we demonstrate that their performance converges towards theerror floor region, owning to the moderate puncturing of their systematicoutput\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Consequently, we propose pseudo-random puncturing as a means ofimproving the bandwidth efficiency of a PCCC and simultaneously lowering itserror floor.\"\n","<https://w3id.org/skg/articles/3544482385407756047> <http://xmlns.com/foaf/0.1/name>\"A Low Complexity Algorithm and Architecture for Systematic Encoding of  Hermitian Codes\".\n","<https://w3id.org/skg/articles/3544482385407756047> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/5993190164001199460>\n","<https://w3id.org/skg/articles/5993190164001199460><http://purl.org/dc/terms/abstract/text>\"  We present an algorithm for systematic encoding of Hermitian codes  For aHermitian code defined over GF(q^2), the proposed algorithm achieves a run timecomplexity of O(q^2) and is suitable for VLSI implementation  The encoderarchitecture uses as main blocks q varying-rate Reed-Solomon encoders andachieves a space complexity of O(q^2) in terms of finite field multipliers andmemory elements.\"\n","<http://purl.org/dc/terms/abstract/OWNX > \"  We present an algorithm for systematic encoding of Hermitian codes\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" For aHermitian code defined over GF(q^2), the proposed algorithm achieves a run timecomplexity of O(q^2) and is suitable for VLSI implementation\"\n","<http://purl.org/dc/terms/abstract/MISC > \" The encoderarchitecture uses as main blocks q varying-rate Reed-Solomon encoders andachieves a space complexity of O(q^2) in terms of finite field multipliers andmemory elements.\"\n","<https://w3id.org/skg/articles/-9205124697652296151> <http://xmlns.com/foaf/0.1/name>\"Learning from compressed observations\".\n","<https://w3id.org/skg/articles/-9205124697652296151> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-8696850721371301559>\n","<https://w3id.org/skg/articles/-8696850721371301559><http://purl.org/dc/terms/abstract/text>\"  The problem of statistical learning is to construct a predictor of a randomvariable $Y$ as a function of a related random variable $X$ on the basis of ani i d  training sample from the joint distribution of $(X,Y)$  Allowablepredictors are drawn from some specified class, and the goal is to approachasymptotically the performance (expected loss) of the best predictor in theclass  We consider the setting in which one has perfect observation of the$X$-part of the sample, while the $Y$-part has to be communicated at somefinite bit rate  The encoding of the $Y$-values is allowed to depend on the$X$-values  Under suitable regularity conditions on the admissible predictors,the underlying family of probability distributions and the loss function, wegive an information-theoretic characterization of achievable predictorperformance in terms of conditional distortion-rate functions  The ideas areillustrated on the example of nonparametric regression in Gaussian noise.\"\n","<http://purl.org/dc/terms/abstract/OWNX > \"  The problem of statistical learning is to construct a predictor of a randomvariable $Y$ as a function of a related random variable $X$ on the basis of ani\"\n","<http://purl.org/dc/terms/abstract/MISC > \"i\"\n","<http://purl.org/dc/terms/abstract/MISC > \"d\"\n","<http://purl.org/dc/terms/abstract/MISC > \" training sample from the joint distribution of $(X,Y)$\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Allowablepredictors are drawn from some specified class, and the goal is to approachasymptotically the performance (expected loss) of the best predictor in theclass\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" We consider the setting in which one has perfect observation of the$X$-part of the sample, while the $Y$-part has to be communicated at somefinite bit rate\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" The encoding of the $Y$-values is allowed to depend on the$X$-values\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" Under suitable regularity conditions on the admissible predictors,the underlying family of probability distributions and the loss function, wegive an information-theoretic characterization of achievable predictorperformance in terms of conditional distortion-rate functions\"\n","<http://purl.org/dc/terms/abstract/OWNX > \" The ideas areillustrated on the example of nonparametric regression in Gaussian noise.\"\n"],"name":"stdout"}]}]}