\documentclass[size=14pt,
  style=tycja,
  paper=screen,
  ]{powerdot}

\title{Neural Networks for Classification}
\author{Andrei Alexandrescu}

\begin{document}

\maketitle

%\tableofcontents

\section{Introduction}

\begin{slide}{Neural Networks: History}
  \vspace{\stretch{1}}
  \begin{itemize}
  \item Modeled after the human brain
  \item Experimentation and marketing predated theory
  \item Considered the forefront of the AI spring
    \subitem Suffered from the AI winter
  \item Theory today still not fully developed and understood
  \end{itemize}
  \vspace{\stretch{1}}
\end{slide}

\begin{slide}{What is a Neural Network?}
  \begin{itemize}
  \item Essentially:
    \subitem \emph{A network of interconnected}
    \subitem \emph{functional elements}
    \subitem \emph{each with several inputs/one output}
  \end{itemize}
  \begin{equation}
    y(x_1,\ldots,x_n) = f(w_1x_1 + w_2x_2 + \ldots + w_nx_n)
  \end{equation}
  \begin{itemize}
    \item $w_i$ are parameters
    \item $f$ is the activation function
    \item Crucial for learning that \emph{addition} is used for integrating the inputs
  \end{itemize}
\end{slide}

\begin{slide}{Examples of Neural Networks}
  \begin{itemize}
  \item Logical functions with 0/1 inputs and outputs
  \item Fourier series:
    \begin{equation}
      F(x) = \displaystyle\sum_{i\geq 0}{(a_i \cos(ix) + b_i \sin(ix))}
    \end{equation}
  \item Taylor series:
    \begin{equation}
      F(x) = \displaystyle\sum_{i\geq 0} a_i(x - x_0)^i
    \end{equation}
  \item Automata
  \end{itemize}
\end{slide}

\begin{slide}{Elements of a Neural Network}
  \begin{itemize}
  \item The function performed by an element
  \item The topology of the network
  \item The method used to train the weights
  \end{itemize}
\end{slide}

\section{Single-Layer Perceptrons}

\begin{slide}{The Perceptron}
  \begin{itemize}
  \item $n$ inputs, one output:\\
  \begin{equation}
    y(x_1,\ldots,x_n) = f(w_1x_1 + \ldots + w_nx_n)
  \end{equation}
  \item Oldest activation function (McCulloch/Pitts):\\
  \begin{equation}
    f(v) = 1_{x \geq 0}(v)
  \end{equation}
  \end{itemize}
\end{slide}

\begin{slide}{Perceptron Capabilities}
  \begin{itemize}
  \item Advertised to be as extensive as the brain itself
  \item Can (only) distinguish between two linearly-separable sets
  \item Smallest undecidable function: XOR 
  \item Minsky's proof started the AI winter
  \item It was not fully understood what connected layers could do
  \end{itemize}
\end{slide}

\begin{slide}{Bias}
  \begin{itemize}
  \item Notice that the decision hyperplane must go through the origin
  \item Could be achieved by preprocessing the input
  \item Not always desirable or possible
  \item Add a bias input:
  \begin{equation}
    y(x_1,\ldots,x_n) = f(w_0 + w_1x_1 + \ldots + w_nx_n)
  \end{equation}
  \item Same as an input connected to the constant $1$
  \item We consider that ghost input implicit henceforth
  \end{itemize}
\end{slide}

\begin{slide}{Training the Perceptron}
  \begin{itemize}
  \item Switch to vector notation:
  \begin{equation}
    y(\mathbf x) = f(\mathbf w \mathbf x) = f_{\mathbf w}(x)
  \end{equation}
  \item Assume we need to separate sets of points $A$ and $B$.
  \begin{equation}
    E(\mathbf w) = \displaystyle\sum_{\mathbf x\in A}(1 - f_{\mathbf w}(\mathbf x)) + \displaystyle\sum_{\mathbf x\in B}f_{\mathbf w}(\mathbf x)
  \end{equation}
  \item Goal: $E(\mathbf w) = 0$
  \item Start from a random $\mathbf w$ and improve it
  \end{itemize}
\end{slide}

\begin{slide}{Algorithm}
  \begin{enumerate}
  \item Start with random $\mathbf w$, set $t = 0$
  \item Select a vector $\mathbf x \in A \cup B$
  \item If $\mathbf x \in A$ and $\mathbf w \mathbf x \leq 0$, then $\mathbf w_{t+1} = \mathbf w_t + \mathbf x$
  \item Else if $\mathbf x \in B$ and $\mathbf w \mathbf x \geq 0$, then $\mathbf w_{t+1} = \mathbf w_t - \mathbf x$    
  \item Conditionally go to step 2
  \end{enumerate}
  \begin{itemize}
   \item Guaranteed to converge \emph{iff} $A$ and $B$ are linearly separable!
  \end{itemize}
\end{slide}

\begin{slide}{Summary of Simple Perceptrons}
  \begin{itemize}
  \item Simple training
  \item Limited capabilities
  %\item Can solve ``inner point'' problem for a polytope
  \item Reasonably efficient training
    \subitem Simplex, linear programming are better
  \end{itemize}
\end{slide}

\section{Multi-Layer Perceptrons}

\begin{slide}{Multi-Layer Perceptrons}
  \begin{itemize}
  \item Let's connect the output of a perceptron to the input of another
  \item What can we compute with this horizontal combination?
  \item (We already take vertical combination for granted)
  \end{itemize}
\end{slide}

\begin{slide}{A Misunderstanding of Epic Proportions}
  \begin{itemize}
  \item Some say ``two-layered'' network
    \begin{itemize}
      \item Two cascaded layers of computational units
    \end{itemize}
  \item Some say ``three-layered'' network
    \begin{itemize}
      \item There is one extra input layer that does nothing
    \end{itemize}
  \item Let's arbitrarily choose ``three-layered''
    \begin{itemize}
      \item Input
      \item Hidden
      \item Output
    \end{itemize}
  \end{itemize}
\end{slide}

\begin{slide}{Workings}
  \begin{itemize}
  \item The hidden layer maps inputs into a second space: ``feature space,'' ``classification space''
  \item This makes the job of the output layer easier
  \end{itemize}
\end{slide}

\begin{slide}{Capabilities}
  \begin{itemize}
  \item Each hidden unit computes a linear separation of the input space
  \item Several hidden units can carve a polytope in the input space
  \item Output units can distinguish polytope membership
  \end{itemize}

  \center $\mathbf \Downarrow$

  \phantom{a}

  Any union of polytopes can be decided
\end{slide}

\begin{slide}{Training Prerequisite}
  \begin{itemize}
  \item The step function bad for gradient descent techniques
  \item Replace with a smooth step function:
  \begin{equation}
    f(v) = \displaystyle\frac{1}{1 + e^{-v}}
  \end{equation}
  \item Notable fact:\\
    $f'(v) = f(v)(1 - f(v))$
  \item Makes the function cycles-friendly
  \end{itemize}
\end{slide}

\begin{slide}{Output Activation}
  \begin{itemize}
  \item Simple binary discrimination---zero-centered sigmoid:
  \begin{equation}
    f(v) = \displaystyle\frac{1 - e^{-v}}{1 + e^{-v}}
  \end{equation}
  \item Probability distribution---softmax:
  \begin{equation}
    f(v_i) = \displaystyle\frac{e^{v_i}}{\displaystyle\sum_{j} e^{v_j}}
  \end{equation}
  \end{itemize}
\end{slide}

\begin{slide}{The Backpropagation Algorithm}
  \begin{itemize}
  \item Works on any differentiable activation function
  \item Gradient descent in weight space
  \item Metaphor: a ball rolls on the error function's envelope
  \item Condition: no flat portion
  \item Ball would stop in indifferent equilibrium
  \item Some add a slight pull term:
  \begin{equation}
    f(v) = \displaystyle\frac{1 - e^{-v}}{1 + e^{-v}} + c v
  \end{equation}
  \end{itemize}
\end{slide}

\begin{slide}{The Task}
  \begin{itemize}
  \item Minimize error function:
    \begin{equation}
      E = \frac{1}{2}\displaystyle\sum_{i=1}^{p}\| o_i - t_i \|^2
    \end{equation}    
    where:
    \begin{itemize}
    \item $o_i$ actual outputs
    \item $t_i$ desired outputs
    \item $p$ number of patterns
    \end{itemize}
    
  \end{itemize}
\end{slide}

\begin{slide}{Training. The Delta Rule}
  \begin{itemize}
  \item Compute $\nabla E = \left(\frac{\partial E}{\partial w_1},\ldots,\frac{\partial E}{\partial w_l}\right)$
  \item Update weights:
    \begin{equation}
      \Delta w_i = -\gamma \frac{\partial E}{\partial w_i} \ \ \ i = 1,\ldots,l
    \end{equation}
  \item Expect to find a point $\nabla E = 0$
  \item Algorithm for computing $\nabla E$: \emph{backpropagation}
  \item Beyond the scope of this class     
  \end{itemize}
\end{slide}

\begin{slide}{Gradient Locality}
  \begin{itemize}
  \item Only summation guarantees locality of backpropagation
  \item Otherwise  backpropagation would  propagate errors due  to one
    input to \emph{all} inputs
  \item Essential to use \emph{summation} as input integration!
  \end{itemize}
\end{slide}

\begin{slide}{Regularization}
  \begin{itemize}
  \item Weights can grow uncontrollably
  \item Add a regularization term that opposes weight growth
    \begin{equation}
      \Delta w_i = -\gamma \frac{\partial E}{\partial w_i} - \alpha w_i
    \end{equation}
  \item Very important practical trick
  \item Also avoids overspecialization
  \item Forces a smoother output
  \end{itemize}
\end{slide}

\begin{slide}{Local Minima}
  \begin{itemize}
  \item The gradient surf can stop in a local minimum
  \item Biggest issue with neural networks
  \item Overspecialization second biggest
  \item Convergence not guaranteed either, but regularization helps
  \end{itemize}
\end{slide}

\section{Accommodating Discrete Inputs}

\begin{slide}{Discrete Inputs}
  \begin{itemize}
  \item Many NLP applications foster discrete features
  \item Neural nets expect real numbers
  \item Smooth: similar outputs for similar inputs
  \item Any two discrete inputs are ``just as different''
  \item Treating them as integral numbers undemocratic
  \end{itemize}
\end{slide}

\begin{slide}{One-Hot Encoding}
  \begin{itemize}
  \item One discrete feature with $n$ values $\rightarrow$ $n$ real inputs
  \item The $i^{th}$ feature value sets the $i^{th}$ input to $1$ and others to $0$
  \item The Hamming distance between any two distinct inputs is now constant!
  \item Disadvantage: input vector size much larger
  \end{itemize}
\end{slide}

\begin{slide}{Optimizing One-Hot Encoding}
  \begin{itemize}
  \item Each hidden unit has all inputs zero except the $i^{th}$ one
  \item Even that one is just multiplied by $1$
  \item Regroup weights by discrete input, not by hidden unit!
  \item Matrix $\mathbf w$ of size $n \times l$
  \item Input $i$ just copies row $i$ to the output (virtual multiplication by $1$)
  \item Cheap computation
  \item Delta rule applies as usual
  \end{itemize}
\end{slide}

\begin{slide}{One-Hot Encoding: Interesting Tidbits}
  \begin{itemize}
  \item The row $\mathbf w_i$ is a continuous representation of discrete feature $i$
  \item Only one row trained per sample
  \item  The  size of  the  continuous  representation  can be  chosen
    depending on the feature's complexity
  \item  Mix  this  continuous  representation freely  with  ``truly''
    continuous features, such as acoustic features
  \end{itemize}
\end{slide}

\section{Outputs}

\begin{slide}{Multi-Label Classification}
  \begin{itemize}
  \item $n$ real outputs summing to $1$
  \item Normalization included in the softmax function:
  \begin{equation}
    f(v_i) = \displaystyle\frac{e^{v_i}}{\displaystyle\sum_{j} e^{v_j}} = \displaystyle\frac{e^{v_i - v_{max}}}{\displaystyle\sum_{j} e^{v_j-v_{max}}}
  \end{equation}
\item   Train   with   $1   -   \epsilon$   for   the   known   label,
  $\frac{\epsilon}{n-1}$ for all others (avoids saturation)
  \end{itemize}
\end{slide}

\begin{slide}{Soft Training}
  \begin{itemize}
  \item Maybe the targets are known probability distribution 
  \item Or want to reduce the number of training cycles
  \item Train with actual desired distributions as desired outputs
  \item Example:  for feature vector  $\mathbf x$, labels  $l_1$, $l_2$,
    $l_3$ are possible with equal probability
  \item   Train  with   $\frac{1  -   \epsilon}{3}$  for   the  three,
    $\frac{\epsilon}{n-3}$ for all others
  \end{itemize}
\end{slide}

\section{NLP Applications}

\begin{slide}{Language Modeling}
  \begin{itemize}
  \item Input: n-gram context
  \item May include arbitrary word features (cool!!!)
  \item Output: probability distribution of next word
  \item Automatically figures which features are important
  \end{itemize}
\end{slide}

\begin{slide}{Lexicon Learning}
  \begin{itemize}
  \item Input: Word-level features (root, stem, morph)
  \item Input: Most frequent previous/next words
  \item Output: Probability distribution of the word's possible POSs
  \end{itemize}
\end{slide}

\begin{slide}{Word Sense Disambiguation}
  \begin{itemize}
  \item Input: bag of words in context, local collocations
  \item Output: Probability distribution over senses
  \end{itemize}
\end{slide}

\section{Conclusions}

\begin{slide}{Conclusions}
  \begin{itemize}
  \item Neural nets respectable machine learning technique
  \item Theory not fully developed
  \item Local optima and overspecialization are killers
  \item Yet can learn very complex functions
  \item Long training time
  \item Short testing time
  \item Small memory requirements
  \end{itemize}
\end{slide}

\end{document}

% \documentclass{beamer}

% \usepackage{beamerthemesplit}

% \title{Example Presentation Created with the Beamer Package}
% \author{Till Tantau}
% \date{\today}

% \begin{document}

% \frame{\titlepage}

% \section[Outline]{}
% \frame{\tableofcontents}

% \section{Introduction}
% \subsection{Overview of the Beamer Class}
% \frame
% {
%   \frametitle{Features of the Beamer Class}

%   \begin{itemize}
%   \item<1-> Normal LaTeX class.
%   \item<2-> Easy overlays.
%   \item<3-> No external programs needed.      
%   \end{itemize}
% }
% \end{document}
    

