{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Biology.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RUUrrwYQNaR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Reading Biology Papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7bJfOphNz2f",
        "colab_type": "code",
        "outputId": "77da7104-f25a-4d80-a27b-3691cf7f4e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## Read the biology papers from 2016 to 2018 and store it in a file\n",
        "import urllib\n",
        "url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=q-bio&from=2016-01-01&until=2018-11-31&metadataPrefix=arXiv' \n",
        "data = urllib.request.urlopen(url).read()\n",
        "\n",
        "bio = open('bio1', 'wb')\n",
        "bio.write(data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2253178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "c8sMHkfZN-e1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Extract the title and abstract from papers - Read from finance1 to finance2\n",
        "\n",
        "!xml_grep 'title|abstract' bio1  > bio2.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RR-pvgZXOD_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Remove Junk lines , here we remove first 3 lines and last 3 lines which are not necessary\n",
        "!cat bio2.txt | tail -n +4 | head -n -3 > bio3.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3VttntpOXqq",
        "colab_type": "code",
        "outputId": "028490f1-2af9-47a3-baa2-ddb05bc21397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## Reading packages for Text classification\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, numpy, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "#import sklearn_crfsuite\n",
        "#from sklearn_crfsuite import scorers\n",
        "#from sklearn_crfsuite import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIJ88IP6Od_-",
        "colab_type": "code",
        "outputId": "b512aa8c-4c7d-434d-b7ea-060180442238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Stopwords import and removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Alxw521Oi3U",
        "colab_type": "code",
        "outputId": "231be22c-6fd5-42ea-8cf5-3d0eddfbcffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "# load the dataset # dataset contains combined labels and text from all training papers\n",
        "data = open('labeled_sentences (1).txt').read()[:-2]\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    #print(content)\n",
        "    labels.append(content[0])\n",
        "    filtered_sentence = [w.lower() for w in content[1:] if not w in stopwords]\n",
        "    texts.append(filtered_sentence)\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "print(trainDF['label'].unique())\n",
        "trainDF.head(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MISC' 'AIMX' 'OWNX' 'CONT' 'BASE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[minimum, description, length, principle, onli...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[underlying, model, class, discrete,, total, e...</td>\n",
              "      <td>MISC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  [minimum, description, length, principle, onli...  MISC\n",
              "1  [underlying, model, class, discrete,, total, e...  MISC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "wNS2hA9nOqil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Used the obtained dataset for training\n",
        "train_x, valid1_x, train_y, valid1_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iI1uBVOOsSR",
        "colab_type": "code",
        "outputId": "cbc514b0-7d4f-4742-da10-bc32c0e3cfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "## Convert from list to string\n",
        "tempp = []\n",
        "\n",
        "for item in train_x:\n",
        "    tempp.append(\" \".join(item))\n",
        "#print(len(train_x))\n",
        "\n",
        "#tempp1 =[]\n",
        "#for item1 in valid_x:\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    \n",
        "#print(len(tempp1))\n",
        "\n",
        "temp =[]\n",
        "temp_len=0\n",
        "for item2 in texts:\n",
        "    temp.append(\" \".join(item2))\n",
        "    temp_len = temp_len+len(texts)\n",
        "print(len(temp))\n",
        "print(temp_len)\n",
        "print(type(temp))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18627\n",
            "346965129\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IAiDB7xpOv8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(temp)\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(tempp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FcKRuowAOzRr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create a classifier\n",
        "import csv\n",
        "trainDF2 = pandas.DataFrame()  \n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    #std_clf = make_pipeline(StandardScaler(with_mean=False), TruncatedSVD(100), MultinominalNB())\n",
        "    #std_clf.fit(feature_vector_train, label)\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    #predictions = classifier.predict(feature_vector_valid)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return predictions\n",
        "    #tt = classifier.predict(feature_vector_valid)\n",
        "    #labels3 = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    #trainDF2['labels'] = labels3\n",
        "    #trainDF2['text']= valid_x\n",
        "    #print(trainDF2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqU9A8xPPAZR",
        "colab_type": "code",
        "outputId": "0c7f688f-233c-418b-9b0c-fc92242e2764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Read title and abstracts and loop through them \n",
        "import re\n",
        "global_list = []\n",
        "title_list =[]\n",
        "\n",
        "test = open(\"bio3.txt\",'r').read().split(\"</abstract>\")\n",
        "#print(test[1])\n",
        "for idx,i in enumerate(test):\n",
        "  title = re.findall(r\"(?<=<title>).*(?=</title>)\",i.replace(\"\\n\",\"\"))\n",
        "  #print(title)\n",
        "  abstract = re.findall(r\"(?<=<abstract>).*\",i.replace(\"\\n\",\"\"))\n",
        "  #print(abstract[0].replace(\"\\n\",\"\"))\n",
        "  nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",abstract[0].replace('\"',\"\").replace('\\n',''))\n",
        "  #temp_abs = re.sub(r\"((?<=[^i]\\.)|\\.(?=[^e]))\",\"\\n\",abstract[0])\n",
        "  #print(abstract)\n",
        "  #temp_str = temp_abs.split(\"\\n\")\n",
        "  #print(temp_str[0])\n",
        "  #print(nlist[1])\n",
        "  global_list.append(nlist)\n",
        "  title_list.append(title)\n",
        "  #print(global_list)\n",
        " \n",
        "  if idx >50:\n",
        "    #print(global_list)\n",
        "    break\n",
        "  #print(abstract[0])\n",
        "  #nlist = re.split(r\"(?:(?<=[^i]\\.)|\\.(?=[^e]))\",str(abstract))\n",
        "  \n",
        "  #print(nlist[1])\n",
        "  \n",
        "  #tempp1 =[]\n",
        "  '''\n",
        "  for idx, item1 in enumerate(nlist):\n",
        "    \n",
        "    if idx > 1 :\n",
        "      break;\n",
        "      print(item1)\n",
        "      tempp1.append(\" \".join(item1))\n",
        "    #print(tempp1)  \n",
        "    \n",
        "    xvalid_count =  count_vect.transform(tempp1)\n",
        "    for item in nlist:\n",
        "      print(item)\n",
        "      valid_x = item\n",
        "      #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "    \n",
        "  '''\n",
        "  #print(global_list[0])\n",
        "  #print(global_list[1])\n",
        "  #print(global_list[2])\n",
        "  #for idx, item1 in enumerate(global_list) :\n",
        "  #  if idx > 1:\n",
        "  #    break\n",
        "  #  print(item1)\n",
        "    #tempp1.append(\" \".join(item1))\n",
        "    #xvalid_count =  count_vect.transform(tempp1)\n",
        "    #accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HePYn1kpPFeR",
        "colab_type": "code",
        "outputId": "e4b9bed7-472b-490f-ae2a-031d71a45e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "## Print triples from data\n",
        "\n",
        "#print(global_list[1])\n",
        "for idx, (item, title) in enumerate(zip(global_list, title_list)):\n",
        "  \n",
        "    \n",
        "\n",
        "  #print(item)\n",
        "  valid_x = item\n",
        "  xvalid_count =  count_vect.transform(valid_x)\n",
        "  accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "  #print(\"\\n\\n\")\n",
        "  if idx>1:\n",
        "    break\n",
        "  \n",
        "  title_id = hash(str(title))\n",
        "  abstract_id = hash(str(item))\n",
        "  line1 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://xmlns.com/foaf/0.1/name>\" + '\"' + \" \".join(title) + '\"' +\".\"\n",
        "  line2 = \"<https://w3id.org/skg/articles/\" + str(title_id) + \"> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/\" + str(abstract_id)+ \">\"\n",
        "  line3 = \"<https://w3id.org/skg/articles/\" + str(abstract_id) +\"><http://purl.org/dc/terms/abstract/text>\" + '\"' + \" \".join(item) + '\"'\n",
        "  print(line1,line2,line3,sep =\"\\n\")\n",
        "  for acc,element in zip(accuracy,item):\n",
        "    print('<http://purl.org/dc/terms/abstract/{} > \"{}\"'.format(acc, element))  \n",
        "    #line4 = (\"<http://purl.org/dc/terms/abstract/\" + str(acc) + \">\" + '\"' + str(element) + '\"' )\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<https://w3id.org/skg/articles/-4079202618155772900> <http://xmlns.com/foaf/0.1/name>\"Primordial Evolution in the Finitary Process Soup\".\n",
            "<https://w3id.org/skg/articles/-4079202618155772900> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/5052414978280071745>\n",
            "<https://w3id.org/skg/articles/5052414978280071745><http://purl.org/dc/terms/abstract/text>\"  A general and basic model of primordial evolution--a soup of reactingfinitary and discrete processes--is employed to identify and analyzefundamental mechanisms that generate and maintain complex structures inprebiotic systems  The processes--$\\epsilon$-machines as defined incomputational mechanics--and their interaction networks both provide welldefined notions of structure  This enables us to quantitatively demonstratehierarchical self-organization in the soup in terms of complexity  We foundthat replicating processes evolve the strategy of successively building higherlevels of organization by autocatalysis  Moreover, this is facilitated by localcomponents that have low structural complexity, but high generality  In effect,the finitary process soup spontaneously evolves a selection pressure thatfavors such components  In light of the finitary process soup's generality,these results suggest a fundamental law of hierarchical systems: globalcomplexity requires local simplicity.\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \"  A general and basic model of primordial evolution--a soup of reactingfinitary and discrete processes--is employed to identify and analyzefundamental mechanisms that generate and maintain complex structures inprebiotic systems\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" The processes--$\\epsilon$-machines as defined incomputational mechanics--and their interaction networks both provide welldefined notions of structure\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" This enables us to quantitatively demonstratehierarchical self-organization in the soup in terms of complexity\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We foundthat replicating processes evolve the strategy of successively building higherlevels of organization by autocatalysis\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" Moreover, this is facilitated by localcomponents that have low structural complexity, but high generality\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" In effect,the finitary process soup spontaneously evolves a selection pressure thatfavors such components\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" In light of the finitary process soup's generality,these results suggest a fundamental law of hierarchical systems: globalcomplexity requires local simplicity.\"\n",
            "<https://w3id.org/skg/articles/1672551164774122412> <http://xmlns.com/foaf/0.1/name>\"Anisotropic probabilistic cellular automaton for a predator-prey system\".\n",
            "<https://w3id.org/skg/articles/1672551164774122412> <http://purl.org/dc/terms/abstract> <http://purl.org/dc/terms/abstract/-3221819674834026290>\n",
            "<https://w3id.org/skg/articles/-3221819674834026290><http://purl.org/dc/terms/abstract/text>\"  We consider a probabilistic cellular automaton to analyze the stochasticdynamics of a predator-prey system  The local rules are Markovian and are basedin the Lotka-Volterra model  The individuals of each species reside on thesites of a lattice and interact with an unsymmetrical neighborhood  We look forthe effect of the space anisotropy in the characterization of the oscillationsof the species population densities  Our study of the probabilistic cellularautomaton is based on simple and pair mean-field approximations and explicitlytakes into account spatial anisotropy.\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \"  We consider a probabilistic cellular automaton to analyze the stochasticdynamics of a predator-prey system\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" The local rules are Markovian and are basedin the Lotka-Volterra model\"\n",
            "<http://purl.org/dc/terms/abstract/MISC > \" The individuals of each species reside on thesites of a lattice and interact with an unsymmetrical neighborhood\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" We look forthe effect of the space anisotropy in the characterization of the oscillationsof the species population densities\"\n",
            "<http://purl.org/dc/terms/abstract/OWNX > \" Our study of the probabilistic cellularautomaton is based on simple and pair mean-field approximations and explicitlytakes into account spatial anisotropy.\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}